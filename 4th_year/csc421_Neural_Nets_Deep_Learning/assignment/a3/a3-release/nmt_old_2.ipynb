{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "TjPTaRB4mpCd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "metadata": {
        "id": "s9IS9B9-yUU5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab_type": "code",
        "outputId": "a6041d05-a56d-42f1-8750-41edce017fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        }
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p /content/csc421/a3/\n",
        "%cd /content/csc421/a3"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python2.7/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.14.6)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "  Using cached https://files.pythonhosted.org/packages/0d/f3/421598450cb9503f4565d936860763b5af413a61009d87a5ab1e34139672/Pillow-5.4.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "\u001b[31mfastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.0.1.post2 which is incompatible.\u001b[0m\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==4.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/89/99/0e3522a9764fe371bf9f7729404b1ef7d9c4fc49cbe5f1761c6e07812345/Pillow-4.0.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mfastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.0.1.post2 which is incompatible.\u001b[0m\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mtorchvision 0.2.2.post3 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 5.4.1\n",
            "    Uninstalling Pillow-5.4.1:\n",
            "      Successfully uninstalled Pillow-5.4.1\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/csc421/a3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9DaTdRNuUra7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Helper code"
      ]
    },
    {
      "metadata": {
        "id": "4BIpGwANoQOg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ]
    },
    {
      "metadata": {
        "id": "D-UJHBYZkh7f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pbvpn4MaV0I1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data loader"
      ]
    },
    {
      "metadata": {
        "id": "XVT4TNTOV3Eg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "\n",
        "    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bRWfRdmVVjUl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "metadata": {
        "id": "wa5-onJhoSeM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "\n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "      \n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "      attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "      fig.colorbar(cax)\n",
        "\n",
        "      # Set up axes\n",
        "      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "      # Show label at every tick\n",
        "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      # Add title\n",
        "      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "      plt.tight_layout()\n",
        "      plt.grid('off')\n",
        "      plt.show()\n",
        "      #plt.savefig(save)\n",
        "\n",
        "      #plt.close(fig)\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "            \n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "              \n",
        "    mean_loss = np.mean(losses)\n",
        "    return mean_loss\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save_loss_plot(train_losses, val_losses, opts)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data()\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    encoder = GRUEncoder(vocab_size=vocab_size, \n",
        "                         hidden_size=opts.hidden_size, \n",
        "                         opts=opts)\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder\n",
        "      \n",
        "    return encoder, decoder\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bXNsLNkOn38w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Your code for NMT models"
      ]
    },
    {
      "metadata": {
        "id": "_BAfi_8yWB3y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GRU cell"
      ]
    },
    {
      "metadata": {
        "id": "9ztmyA5Ro67o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # Input linear layers\n",
        "        self.Wiz = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.Wir = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.Wih = nn.Linear(self.input_size, self.hidden_size)\n",
        "\n",
        "        # Hidden linear layers\n",
        "        self.Whz = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.Whr = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.Whh = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        z = F.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
        "        r = F.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
        "        g = F.tanh(self.Wih(x) + r * self.Whh(h_prev))\n",
        "        h_new = (1 - z) * g + z * h_prev\n",
        "        return h_new\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-JBVFLEZWNC1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### GRU encoder / decoder"
      ]
    },
    {
      "metadata": {
        "id": "xaDt7XDmWRzC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)\n",
        "\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tWe0RO5FWajD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ]
    },
    {
      "metadata": {
        "id": "9GUK5A7CWhV8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size, seq_len, hidden_size = keys.size()\n",
        "        expanded_queries = queries.unsqueeze(1).expand_as(keys)\n",
        "        concat_inputs = torch.cat((expanded_queries, keys), 2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs.view(batch_size * seq_len, 2 * hidden_size)).view(batch_size, seq_len, 1) \n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.view(batch_size, 1, seq_len), values)\n",
        "        return context, attention_weights\n",
        "        \n",
        "      \n",
        "\n",
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size, seq_len, hidden_size = keys.size()\n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        \n",
        "        if len(q.size()) == 2:\n",
        "            q = q.view(batch_size, 1, hidden_size)\n",
        "            \n",
        "        unnormalized_attention = torch.bmm(k, torch.transpose(q,1,2)) * self.scaling_factor\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(torch.transpose(attention_weights,1,2), v)\n",
        "        return context, attention_weights\n",
        "        \n",
        "\n",
        "      \n",
        "      \n",
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size, seq_len, hidden_size = keys.size()\n",
        "        q = self.Q(queries)\n",
        "        k = self.K(keys)\n",
        "        v = self.V(values)\n",
        "        \n",
        "        if len(q.size()) == 2:\n",
        "            q = q.view(batch_size, 1, hidden_size)\n",
        "        k_size = q.size()[1]\n",
        "        \n",
        "        unnormalized_attention = torch.bmm(k, torch.transpose(q,1,2)) * self.scaling_factor\n",
        "        mask = torch.tril(torch.ones((seq_len, k_size), dtype=torch.uint8), diagonal=k_size-seq_len).cuda().unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        unnormalized_attention = unnormalized_attention.masked_fill(mask, self.neg_inf)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(torch.transpose(attention_weights,1,2), v)\n",
        "        return context, attention_weights\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pemjZo2XWtRt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Attention decoder"
      ]
    },
    {
      "metadata": {
        "id": "PfjF0Z-PWwPv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            # ------------\n",
        "            # FILL THIS IN\n",
        "            # ------------\n",
        "            embed_current = embed[:,i,:]\n",
        "            context, attention_weights = self.attention.forward(h_prev, annotations, annotations)\n",
        "            embed_and_context = torch.cat((context.squeeze(1), embed_current), 1) \n",
        "            h_prev = self.rnn.forward(embed_and_context, h_prev)\n",
        "\n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N8JpcwTRW5cw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformer decoder"
      ]
    },
    {
      "metadata": {
        "id": "V5vJPku1W7sz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "          # ------------\n",
        "          # FILL THIS IN\n",
        "          # ------------\n",
        "          new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)\n",
        "          residual_contexts = contexts + new_contexts\n",
        "          new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations)\n",
        "          residual_contexts = residual_contexts + new_contexts\n",
        "          new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "          contexts = residual_contexts + new_contexts + contexts\n",
        "\n",
        "          \n",
        "          encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "          self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XuNFd6LNo0-o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training\n"
      ]
    },
    {
      "metadata": {
        "id": "kiUwiOITHTW4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download dataset"
      ]
    },
    {
      "metadata": {
        "id": "xwcFjsEpHRbI",
        "colab_type": "code",
        "outputId": "121a12f3-d4b4-4956-a631-d3af735d0aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_data.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hmQmyJDSRFKR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNN decoder"
      ]
    },
    {
      "metadata": {
        "id": "0LKaRF1jwhH7",
        "colab_type": "code",
        "outputId": "f2322141-e055-45ad-a47d-112ea94a577b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2213
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_encoder, rnn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: rnn                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.403 | Val loss: 1.993 | Gen: ensay ay enseday ensay enseday\n",
            "Epoch:   1 | Train loss: 1.922 | Val loss: 1.822 | Gen: ereday ateray ontereday eray onteray\n",
            "Epoch:   2 | Train loss: 1.770 | Val loss: 1.729 | Gen: eray artedtay onteredsay ingsay onterstay\n",
            "Epoch:   3 | Train loss: 1.671 | Val loss: 1.663 | Gen: eray astedsay ontingeday ingsay onterstay\n",
            "Epoch:   4 | Train loss: 1.591 | Val loss: 1.598 | Gen: eray astionssay ontingeday ingsay ontingsay\n",
            "Epoch:   5 | Train loss: 1.514 | Val loss: 1.556 | Gen: eray asionssay ontingray illay ortersay\n",
            "Epoch:   6 | Train loss: 1.455 | Val loss: 1.506 | Gen: eray aingray ontionssay illay ortedway\n",
            "Epoch:   7 | Train loss: 1.401 | Val loss: 1.476 | Gen: eray aingray ontionday isthay ortedway\n",
            "Epoch:   8 | Train loss: 1.353 | Val loss: 1.447 | Gen: eray aisthay ontionday istionday ortingsay\n",
            "Epoch:   9 | Train loss: 1.318 | Val loss: 1.425 | Gen: eray aisthay oningstay isstay ortingsay\n",
            "Epoch:  10 | Train loss: 1.299 | Val loss: 1.422 | Gen: eray aisthay oningstay isstay ortingsay\n",
            "Epoch:  11 | Train loss: 1.256 | Val loss: 1.391 | Gen: eray aisthay oningstay issstay ortingsray\n",
            "Epoch:  12 | Train loss: 1.230 | Val loss: 1.395 | Gen: eray aisshay oningstionsway isstay ortingsray\n",
            "Epoch:  13 | Train loss: 1.207 | Val loss: 1.379 | Gen: eray aisthay onincordway issstay oringsray\n",
            "Epoch:  14 | Train loss: 1.202 | Val loss: 1.373 | Gen: eray aisthay oningstionsway issstay orsinglay\n",
            "Epoch:  15 | Train loss: 1.182 | Val loss: 1.333 | Gen: eray aisshay onincondway isstay oringlay\n",
            "Epoch:  16 | Train loss: 1.147 | Val loss: 1.301 | Gen: eray aisthay oninconssay isstay oringlay\n",
            "Epoch:  17 | Train loss: 1.121 | Val loss: 1.299 | Gen: eway aisthay oninconssay isthay oringlay\n",
            "Epoch:  18 | Train loss: 1.105 | Val loss: 1.296 | Gen: eway airdway oninconssay isthay oringlay\n",
            "Epoch:  19 | Train loss: 1.090 | Val loss: 1.286 | Gen: ehay airdway oncingstionsway istway oringlay\n",
            "Epoch:  20 | Train loss: 1.079 | Val loss: 1.286 | Gen: ehay airdway oninconssay istway orintlysay\n",
            "Epoch:  21 | Train loss: 1.070 | Val loss: 1.276 | Gen: ehay airdway oninconedway istway oringlay\n",
            "Epoch:  22 | Train loss: 1.063 | Val loss: 1.265 | Gen: ehtray airdway oninconedway istway unsistionsway\n",
            "Epoch:  23 | Train loss: 1.055 | Val loss: 1.271 | Gen: ehay airdway onincionsway istway unsistionsway\n",
            "Epoch:  24 | Train loss: 1.042 | Val loss: 1.284 | Gen: ehray airdway oncingstedway istway unsistionsway\n",
            "Epoch:  25 | Train loss: 1.035 | Val loss: 1.246 | Gen: ehtray airdway oncingingsay istway uncilylay\n",
            "Epoch:  26 | Train loss: 1.053 | Val loss: 1.253 | Gen: ehtay airway onincionsway isthay unctionssay\n",
            "Epoch:  27 | Train loss: 1.023 | Val loss: 1.236 | Gen: ehtray airdway onincesspay istway uncilylay\n",
            "Epoch:  28 | Train loss: 0.996 | Val loss: 1.232 | Gen: ehtray airdway onincionsway istway unctionssay\n",
            "Epoch:  29 | Train loss: 0.978 | Val loss: 1.224 | Gen: ehtray airway onincionsway istway unctionssay\n",
            "Epoch:  30 | Train loss: 0.974 | Val loss: 1.231 | Gen: ehtray airway onincesspay istway uncivitay\n",
            "Epoch:  31 | Train loss: 0.961 | Val loss: 1.220 | Gen: ehtray ariblyhay onincionsway istway unctionssay\n",
            "Epoch:  32 | Train loss: 0.959 | Val loss: 1.228 | Gen: ehtray airway onincionsray istway uncivitionday\n",
            "Epoch:  33 | Train loss: 0.965 | Val loss: 1.218 | Gen: ehtray airway onincedway istway uncivitay\n",
            "Epoch:  34 | Train loss: 0.950 | Val loss: 1.213 | Gen: ehay airway onincionsway istway uncivoway\n",
            "Epoch:  35 | Train loss: 0.942 | Val loss: 1.237 | Gen: ehtray airway onincioncay istway uncivoway\n",
            "Epoch:  36 | Train loss: 0.962 | Val loss: 1.214 | Gen: ehay airway onincioncay istway uncivoway\n",
            "Epoch:  37 | Train loss: 0.934 | Val loss: 1.190 | Gen: ehay airway onincioncay istway orkingray\n",
            "Epoch:  38 | Train loss: 0.912 | Val loss: 1.187 | Gen: ehay airdway onincioncay istway uncivoway\n",
            "Epoch:  39 | Train loss: 0.903 | Val loss: 1.175 | Gen: ehay ariy onincioncay istway uncivoway\n",
            "Epoch:  40 | Train loss: 0.897 | Val loss: 1.177 | Gen: ehay airdway onincioncay istway orkingray\n",
            "Epoch:  41 | Train loss: 0.885 | Val loss: 1.176 | Gen: ehay airway onincioncay istway uncivoway\n",
            "Epoch:  42 | Train loss: 0.890 | Val loss: 1.185 | Gen: ehtway airdtay onincioncay istway uncivoway\n",
            "Epoch:  43 | Train loss: 0.900 | Val loss: 1.191 | Gen: eway-awlay airway onincioncay istway uncivoway\n",
            "Epoch:  44 | Train loss: 0.879 | Val loss: 1.170 | Gen: ehay airway onincioncay istway uncivoway\n",
            "Epoch:  45 | Train loss: 0.867 | Val loss: 1.180 | Gen: ehay airway onincioncay istway uncivoway\n",
            "Epoch:  46 | Train loss: 0.880 | Val loss: 1.176 | Gen: ehay airway onincioncay istway orkingray\n",
            "Epoch:  47 | Train loss: 0.942 | Val loss: 1.202 | Gen: ehay airway onincioncay istway orkingray\n",
            "Epoch:  48 | Train loss: 0.901 | Val loss: 1.147 | Gen: ehay airway onincioncay istway orkingray\n",
            "Epoch:  49 | Train loss: 0.854 | Val loss: 1.136 | Gen: ehay airway onincioncay istway unciveday\n",
            "Epoch:  50 | Train loss: 0.842 | Val loss: 1.143 | Gen: ehay airway onincioncay istway orkingray\n",
            "Epoch:  51 | Train loss: 0.829 | Val loss: 1.148 | Gen: ehay airway onincioncay istway orkingray\n",
            "Epoch:  52 | Train loss: 0.828 | Val loss: 1.147 | Gen: ehay airway onincioncay istway orkingray\n",
            "Epoch:  53 | Train loss: 0.828 | Val loss: 1.151 | Gen: ehay airway ondingioncay istway orkingray\n",
            "Epoch:  54 | Train loss: 0.838 | Val loss: 1.163 | Gen: ehay airway onincioncay istway orkingray\n",
            "Epoch:  55 | Train loss: 0.829 | Val loss: 1.127 | Gen: ehay airway ondingioncay istway uncivoway\n",
            "Epoch:  56 | Train loss: 0.817 | Val loss: 1.132 | Gen: ehay airway ondingioncay istway orkingray\n",
            "Epoch:  57 | Train loss: 0.813 | Val loss: 1.149 | Gen: ehay airway ondincecay istway orkingray\n",
            "Epoch:  58 | Train loss: 0.806 | Val loss: 1.144 | Gen: ehay airsway ondingingray istway orkingray\n",
            "Epoch:  59 | Train loss: 0.806 | Val loss: 1.131 | Gen: ehay airway ondinconsway istway orkingray\n",
            "Epoch:  60 | Train loss: 0.795 | Val loss: 1.128 | Gen: ehay airway ondincecay istway orkingray\n",
            "Epoch:  61 | Train loss: 0.809 | Val loss: 1.163 | Gen: ehay airway ondingsray istway orkingray\n",
            "Epoch:  62 | Train loss: 0.800 | Val loss: 1.141 | Gen: ehay airway ondincecay istway orkndtyway\n",
            "Epoch:  63 | Train loss: 0.793 | Val loss: 1.148 | Gen: ehay airway ondonicedray istway orkndtyway\n",
            "Epoch:  64 | Train loss: 0.791 | Val loss: 1.123 | Gen: ehay airway ondincoredway istway orkingray\n",
            "Epoch:  65 | Train loss: 0.782 | Val loss: 1.135 | Gen: ehay airway ondinconsway istway orkndtyway\n",
            "Epoch:  66 | Train loss: 0.781 | Val loss: 1.130 | Gen: ehay airway ondincecay isway orkndtyway\n",
            "Epoch:  67 | Train loss: 0.771 | Val loss: 1.128 | Gen: ehay airway ondincecay isway orkingray\n",
            "Epoch:  68 | Train loss: 0.779 | Val loss: 1.161 | Gen: ehay airway onincioncay istway orkndtyway\n",
            "Epoch:  69 | Train loss: 0.776 | Val loss: 1.118 | Gen: ehay airway ondincecay isway orkndtyway\n",
            "Epoch:  70 | Train loss: 0.775 | Val loss: 1.133 | Gen: ehay airway onincioncay isway orkndtray\n",
            "Epoch:  71 | Train loss: 0.805 | Val loss: 1.096 | Gen: ehay airway ondincecay istway orkningway\n",
            "Epoch:  72 | Train loss: 0.775 | Val loss: 1.092 | Gen: ehay airway ondincounsray isway orknionsay\n",
            "Epoch:  73 | Train loss: 0.760 | Val loss: 1.098 | Gen: ehay airway onincioncay isway orkndtyway\n",
            "Epoch:  74 | Train loss: 0.749 | Val loss: 1.109 | Gen: ehay airway onincioncay isway orkndtyway\n",
            "Epoch:  75 | Train loss: 0.742 | Val loss: 1.098 | Gen: ehay airway onincioncay isway orkndtyway\n",
            "Epoch:  76 | Train loss: 0.733 | Val loss: 1.115 | Gen: ehay airway onindioncay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.735 | Val loss: 1.115 | Gen: ehay airway onincioncay isway orkndtyway\n",
            "Epoch:  78 | Train loss: 0.727 | Val loss: 1.108 | Gen: ehay airway onincioncay isway orkingray\n",
            "Epoch:  79 | Train loss: 0.741 | Val loss: 1.109 | Gen: ehay airway onincioncay isway orkndtyway\n",
            "Epoch:  80 | Train loss: 0.739 | Val loss: 1.113 | Gen: ehay airway onincioncay isway orkndtyway\n",
            "Epoch:  81 | Train loss: 0.757 | Val loss: 1.124 | Gen: ehay airway onincioncay isway orknionway\n",
            "Epoch:  82 | Train loss: 0.735 | Val loss: 1.104 | Gen: ehay airway onincioncay isway orknionsay\n",
            "Epoch:  83 | Train loss: 0.715 | Val loss: 1.093 | Gen: ehay airway onincioncay isway orkingray\n",
            "Epoch:  84 | Train loss: 0.714 | Val loss: 1.109 | Gen: ehay airway onincioncay isway orkndtyway\n",
            "Epoch:  85 | Train loss: 0.703 | Val loss: 1.102 | Gen: ehay airway onincioncay isway orkndtyway\n",
            "Epoch:  86 | Train loss: 0.714 | Val loss: 1.107 | Gen: ehay airway onincioncay isway orkndtyway\n",
            "Epoch:  87 | Train loss: 0.714 | Val loss: 1.136 | Gen: ehay airway onincioncay isway orknionway\n",
            "Epoch:  88 | Train loss: 0.705 | Val loss: 1.104 | Gen: ehay airway onincioncay isway orkndstay\n",
            "Epoch:  89 | Train loss: 0.694 | Val loss: 1.109 | Gen: ehay airway onincioncay isway orknionway\n",
            "Epoch:  90 | Train loss: 0.690 | Val loss: 1.114 | Gen: ehay airway onincioncay isway orknionway\n",
            "Epoch:  91 | Train loss: 0.695 | Val loss: 1.150 | Gen: ehay airway onincioncay isway orkniblyay\n",
            "Epoch:  92 | Train loss: 0.692 | Val loss: 1.112 | Gen: ehay airway onincionscay isway orkndtyway\n",
            "Epoch:  93 | Train loss: 0.688 | Val loss: 1.130 | Gen: ehay airway onincioncay isway orknionway\n",
            "Epoch:  94 | Train loss: 0.685 | Val loss: 1.160 | Gen: ehay airway onincioncay isway orkniblyay\n",
            "Epoch:  95 | Train loss: 0.706 | Val loss: 1.130 | Gen: ehay airway onincionscay isway orescesay\n",
            "Epoch:  96 | Train loss: 0.703 | Val loss: 1.079 | Gen: ehay airway onincionday isway orkndtyway\n",
            "Epoch:  97 | Train loss: 0.681 | Val loss: 1.120 | Gen: ehay airway oninciousday isway orkniblay\n",
            "Epoch:  98 | Train loss: 0.675 | Val loss: 1.098 | Gen: ehay airway oninciousday isway orkniblay\n",
            "Epoch:  99 | Train loss: 0.669 | Val loss: 1.111 | Gen: ehay airway oninciousday isway orkingray\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehay airway oninciousday isway orkingray\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p2kPGj5DFv7a",
        "colab_type": "code",
        "outputId": "2bad923f-cb2f-433e-e4bc-0c8b05f59b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehay airway oninciousday isway orkingray\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7cP7nl5NRJbu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNN attention decoder"
      ]
    },
    {
      "metadata": {
        "id": "nKlyfbuPDXDR",
        "colab_type": "code",
        "outputId": "85e492aa-aaa6-4f67-bb96-1fcb4e05c519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2213
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: rnn_attention                          \n",
            "                               lr_decay: 0.99                                   \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.357 | Val loss: 1.948 | Gen: estay eseray ongay ay ongay\n",
            "Epoch:   1 | Train loss: 1.849 | Val loss: 1.736 | Gen: ereray aray ongay erestay onghay\n",
            "Epoch:   2 | Train loss: 1.634 | Val loss: 1.559 | Gen: erthay aray onghay iseday onghay\n",
            "Epoch:   3 | Train loss: 1.453 | Val loss: 1.398 | Gen: eay aillay ontinghay isedday oogtinggay\n",
            "Epoch:   4 | Train loss: 1.306 | Val loss: 1.281 | Gen: eray aillay oitingay isedday orfinggray\n",
            "Epoch:   5 | Train loss: 1.190 | Val loss: 1.129 | Gen: eay aillay oictingay issay orcingcay\n",
            "Epoch:   6 | Train loss: 1.008 | Val loss: 1.020 | Gen: eay aillay ontiongay issray orcingcay\n",
            "Epoch:   7 | Train loss: 0.913 | Val loss: 0.909 | Gen: eay ailay onditiongay issay orkingcay\n",
            "Epoch:   8 | Train loss: 0.775 | Val loss: 0.791 | Gen: eray airray onditiongay issay orkingway\n",
            "Epoch:   9 | Train loss: 0.682 | Val loss: 0.730 | Gen: emay airray onditiongay issay orkingcay\n",
            "Epoch:  10 | Train loss: 0.629 | Val loss: 0.673 | Gen: eay airray onditiongay issay orkingway\n",
            "Epoch:  11 | Train loss: 0.581 | Val loss: 0.828 | Gen: emhay airray onditingay issay orkingway\n",
            "Epoch:  12 | Train loss: 0.553 | Val loss: 0.572 | Gen: ehay airray onditiongay issay orkingway\n",
            "Epoch:  13 | Train loss: 0.578 | Val loss: 0.587 | Gen: epay airay onditiongay ispay orkingway\n",
            "Epoch:  14 | Train loss: 0.459 | Val loss: 0.519 | Gen: ethay airray onditiongay issay orkingcay\n",
            "Epoch:  15 | Train loss: 0.458 | Val loss: 0.499 | Gen: ehthay airay onditiongay iscay orkingcay\n",
            "Epoch:  16 | Train loss: 0.388 | Val loss: 0.556 | Gen: ehtay airay ondititingingway iscay orkingway\n",
            "Epoch:  17 | Train loss: 0.420 | Val loss: 0.433 | Gen: ehay airray onditiongay issay orkingway\n",
            "Epoch:  18 | Train loss: 0.403 | Val loss: 0.514 | Gen: etay airay onditioningway isay orkingway\n",
            "Epoch:  19 | Train loss: 0.359 | Val loss: 0.414 | Gen: ehay-ttay airay onditiongay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.333 | Val loss: 0.385 | Gen: ehatththay airay onditiongay isway orkingay\n",
            "Epoch:  21 | Train loss: 0.324 | Val loss: 0.694 | Gen: eptay airalway onditioningay isaslay orkingay\n",
            "Epoch:  22 | Train loss: 0.399 | Val loss: 0.373 | Gen: epay airray onditiongnay isay orkingway\n",
            "Epoch:  23 | Train loss: 0.276 | Val loss: 0.328 | Gen: epthay airway onditiongangday isay orkingway\n",
            "Epoch:  24 | Train loss: 0.247 | Val loss: 0.354 | Gen: ehay-tthay airway onditiongay isay orkingway\n",
            "Epoch:  25 | Train loss: 0.234 | Val loss: 0.345 | Gen: epthay airway onditiongnay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.227 | Val loss: 0.463 | Gen: ethay airway onditiongay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.345 | Val loss: 0.436 | Gen: ethay airway onditiongway issay orkingway\n",
            "Epoch:  28 | Train loss: 0.323 | Val loss: 0.476 | Gen: ethay airway onditiongway issay orkingway\n",
            "Epoch:  29 | Train loss: 0.265 | Val loss: 0.273 | Gen: etay-isththay airray onditiongway isay-esay orkingway\n",
            "Epoch:  30 | Train loss: 0.214 | Val loss: 0.266 | Gen: etay airway onditiongay isay orkingway\n",
            "Epoch:  31 | Train loss: 0.173 | Val loss: 0.226 | Gen: ethay airway onditiongway isay orkingway\n",
            "Epoch:  32 | Train loss: 0.150 | Val loss: 0.248 | Gen: ethay airway onditiongway isay-esay orkingway\n",
            "Epoch:  33 | Train loss: 0.136 | Val loss: 0.195 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  34 | Train loss: 0.115 | Val loss: 0.181 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  35 | Train loss: 0.114 | Val loss: 0.245 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.114 | Val loss: 0.180 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  37 | Train loss: 0.107 | Val loss: 0.172 | Gen: ethay airway onditiongway isway orkingway\n",
            "Epoch:  38 | Train loss: 0.103 | Val loss: 0.246 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.098 | Val loss: 0.156 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.081 | Val loss: 0.153 | Gen: ethay airway onditiongay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.099 | Val loss: 0.306 | Gen: ethay airway onditiongcay isway orkingnway\n",
            "Epoch:  42 | Train loss: 0.185 | Val loss: 0.269 | Gen: ethay airiway onditiongway isay orkingway\n",
            "Epoch:  43 | Train loss: 0.108 | Val loss: 0.174 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.122 | Val loss: 0.236 | Gen: ethay airway onditiongcay isay-ay orkingway\n",
            "Epoch:  45 | Train loss: 0.144 | Val loss: 0.197 | Gen: ethay airway onditiongcay isaway orkingway\n",
            "Epoch:  46 | Train loss: 0.088 | Val loss: 0.140 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.054 | Val loss: 0.117 | Gen: ethay airway onditiongncay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.047 | Val loss: 0.124 | Gen: ethay airway onditiongnay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.040 | Val loss: 0.135 | Gen: ethay airway onditioningcay isay orkingway\n",
            "Epoch:  50 | Train loss: 0.036 | Val loss: 0.099 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.033 | Val loss: 0.136 | Gen: ethay airway onditioningcay isay orkingway\n",
            "Epoch:  52 | Train loss: 0.044 | Val loss: 0.128 | Gen: ethay airway onditioningcay isay orkingway\n",
            "Epoch:  53 | Train loss: 0.043 | Val loss: 0.116 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.067 | Val loss: 0.188 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.094 | Val loss: 0.209 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.087 | Val loss: 0.154 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.087 | Val loss: 0.245 | Gen: ethay airway onditiongcay isay orkingway\n",
            "Epoch:  58 | Train loss: 0.064 | Val loss: 0.098 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.033 | Val loss: 0.085 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.022 | Val loss: 0.081 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.017 | Val loss: 0.078 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.014 | Val loss: 0.077 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.012 | Val loss: 0.077 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.011 | Val loss: 0.076 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.010 | Val loss: 0.078 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.009 | Val loss: 0.077 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.008 | Val loss: 0.075 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.008 | Val loss: 0.076 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.007 | Val loss: 0.072 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.006 | Val loss: 0.073 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.006 | Val loss: 0.071 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.006 | Val loss: 0.071 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.005 | Val loss: 0.070 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.005 | Val loss: 0.068 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.005 | Val loss: 0.068 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.004 | Val loss: 0.067 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.006 | Val loss: 0.110 | Gen: ethay airway onditionicggcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.271 | Val loss: 0.440 | Gen: ethay airway onditiningcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.215 | Val loss: 0.199 | Gen: ethayfay airway onditiongcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.076 | Val loss: 0.149 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.034 | Val loss: 0.103 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.018 | Val loss: 0.093 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.013 | Val loss: 0.083 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.010 | Val loss: 0.082 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.009 | Val loss: 0.082 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.008 | Val loss: 0.080 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.007 | Val loss: 0.079 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.006 | Val loss: 0.078 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.006 | Val loss: 0.077 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.005 | Val loss: 0.074 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.005 | Val loss: 0.074 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.005 | Val loss: 0.073 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.004 | Val loss: 0.071 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.004 | Val loss: 0.070 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.004 | Val loss: 0.071 | Gen: ethay airway onditionicgnay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.003 | Val loss: 0.069 | Gen: ethay airway onditionicgnay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.003 | Val loss: 0.068 | Gen: ethay airway onditionicgnay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.003 | Val loss: 0.067 | Gen: ethay airway onditionicgnay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.003 | Val loss: 0.067 | Gen: ethay airway onditionicgnay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditionicgnay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vE-hKCxhF3iR",
        "colab_type": "code",
        "outputId": "ba525472-abf4-45ca-eaa2-d51d96c594bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X8FaZZUWRpY9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ]
    },
    {
      "metadata": {
        "id": "Ik5rx9qw9KCg",
        "colab_type": "code",
        "outputId": "ffcf65cb-99f5-4bbb-d831-1ec498389c1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2213
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "transformer_encoder, transformer_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                            hidden_size: 20                                     \n",
            "                          learning_rate: 0.005                                  \n",
            "                 num_transformer_layers: 3                                      \n",
            "                             batch_size: 64                                     \n",
            "                                nepochs: 100                                    \n",
            "                                   cuda: 1                                      \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                           decoder_type: transformer                            \n",
            "                               lr_decay: 0.99                                   \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('payment', 'aymentpay')\n",
            "('ordination', 'ordinationway')\n",
            "('amends', 'amendsway')\n",
            "('principally', 'incipallypray')\n",
            "('anybody', 'anybodyway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.851 | Val loss: 1.824 | Gen: esseyy aray ongay issay oungway\n",
            "Epoch:   1 | Train loss: 1.635 | Val loss: 1.396 | Gen: exhehey aray onconcoycmy isssay ougtay\n",
            "Epoch:   2 | Train loss: 1.260 | Val loss: 1.102 | Gen: eheheheheheheheheheh airay ooccoyciy issisay orwingy\n",
            "Epoch:   3 | Train loss: 0.948 | Val loss: 0.927 | Gen: ehthehay airshay onditingiongiondiong issay orgingingway\n",
            "Epoch:   4 | Train loss: 0.778 | Val loss: 0.781 | Gen: ehthay airshay ongiongiongcay isgay orgingusay\n",
            "Epoch:   5 | Train loss: 0.674 | Val loss: 0.768 | Gen: ehay airway ongiondiongcay isgay orkingkingy\n",
            "Epoch:   6 | Train loss: 0.547 | Val loss: 0.637 | Gen: ehthay airay ondiongcay issway oringwayEOSvvay\n",
            "Epoch:   7 | Train loss: 0.445 | Val loss: 0.545 | Gen: ehay airwayy ondingcay issgay orkghgway\n",
            "Epoch:   8 | Train loss: 0.377 | Val loss: 0.486 | Gen: ehay airy oningcay isway orkgay\n",
            "Epoch:   9 | Train loss: 0.334 | Val loss: 0.423 | Gen: ehay airy ondiongcay isway orkingay\n",
            "Epoch:  10 | Train loss: 0.343 | Val loss: 0.525 | Gen: thEOSaaEOSyyyyyyyyy airy ondiongcay isway orkingway\n",
            "Epoch:  11 | Train loss: 0.352 | Val loss: 0.449 | Gen: ehay airy ondingcay isway orkingway\n",
            "Epoch:  12 | Train loss: 0.314 | Val loss: 0.429 | Gen: ethyyy airy ondioniningcay isway orkingay\n",
            "Epoch:  13 | Train loss: 0.258 | Val loss: 0.370 | Gen: ehhhyy airwayy ondininingcay isway orkingingay\n",
            "Epoch:  14 | Train loss: 0.248 | Val loss: 0.299 | Gen: ethayy airway ondioniningcay isway orkingingway\n",
            "Epoch:  15 | Train loss: 0.196 | Val loss: 0.308 | Gen: ethayy airwayyy onditiongcay isway orkingingway\n",
            "Epoch:  16 | Train loss: 0.196 | Val loss: 0.384 | Gen: ethay airayEOSy onditiondindindindin isway orkingay\n",
            "Epoch:  17 | Train loss: 0.252 | Val loss: 0.392 | Gen: ehhayy aiiwayyy onditingcay isway orkingay\n",
            "Epoch:  18 | Train loss: 0.222 | Val loss: 0.286 | Gen: etty aiwaay onditingcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.178 | Val loss: 0.220 | Gen: ttty aiaaayy onditingcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.112 | Val loss: 0.227 | Gen: ethy airrway onditingcayy isway orkingway\n",
            "Epoch:  21 | Train loss: 0.119 | Val loss: 0.203 | Gen: tthytty airwaay onditingcayy iswayy orkingway\n",
            "Epoch:  22 | Train loss: 0.094 | Val loss: 0.193 | Gen: ethayEOSayy airwaay onditingcayy isway orkingway\n",
            "Epoch:  23 | Train loss: 0.096 | Val loss: 0.222 | Gen: ethyy airrway onditingccy isway orkingway\n",
            "Epoch:  24 | Train loss: 0.129 | Val loss: 0.215 | Gen: ethyy airwaay onditingcny isway orkingway\n",
            "Epoch:  25 | Train loss: 0.141 | Val loss: 0.239 | Gen: etay airwaay onditinccyy isway orkingway\n",
            "Epoch:  26 | Train loss: 0.155 | Val loss: 0.263 | Gen: ethay airwy onditingcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.135 | Val loss: 0.220 | Gen: ethEOSy airwy ondibingccy isway orkingway\n",
            "Epoch:  28 | Train loss: 0.095 | Val loss: 0.185 | Gen: ethay airry onditingcayyy isway orkingway\n",
            "Epoch:  29 | Train loss: 0.089 | Val loss: 0.181 | Gen: ethay airaayy onditingccy isway orkingingway\n",
            "Epoch:  30 | Train loss: 0.068 | Val loss: 0.152 | Gen: ethay airay onditingay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.069 | Val loss: 0.183 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.094 | Val loss: 0.209 | Gen: eaay airway onditincay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.087 | Val loss: 0.195 | Gen: tthyy airwy onditingcyy isway orkingway\n",
            "Epoch:  34 | Train loss: 0.105 | Val loss: 0.207 | Gen: ethay airay onditingcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.067 | Val loss: 0.210 | Gen: ethay airwy onditingcayy isway orkingway\n",
            "Epoch:  36 | Train loss: 0.063 | Val loss: 0.150 | Gen: ethy airay onditingcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.061 | Val loss: 0.156 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.075 | Val loss: 0.170 | Gen: ethay airry onditnncccy isway orkingway\n",
            "Epoch:  39 | Train loss: 0.067 | Val loss: 0.168 | Gen: ethay airwaEOSry onditingcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.169 | Val loss: 0.335 | Gen: ey airy ondiongcay isay orkingway\n",
            "Epoch:  41 | Train loss: 0.229 | Val loss: 0.304 | Gen: ethay airway ondiongcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.311 | Val loss: 0.362 | Gen: - arrry ondioogcny iswaay orkingwagway\n",
            "Epoch:  43 | Train loss: 0.169 | Val loss: 0.231 | Gen: - arrry - iswayyy orkingway\n",
            "Epoch:  44 | Train loss: 0.100 | Val loss: 0.237 | Gen: - arrry onditingiy isway orkingwayEOSvy\n",
            "Epoch:  45 | Train loss: 0.100 | Val loss: 0.187 | Gen: -- arrry onditingay iswayy orkingwy\n",
            "Epoch:  46 | Train loss: 0.112 | Val loss: 0.187 | Gen: - irway onditingcay isway orkingwyy\n",
            "Epoch:  47 | Train loss: 0.055 | Val loss: 0.159 | Gen: ethy array onditingcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.061 | Val loss: 0.164 | Gen: - airayay onditingcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.043 | Val loss: 0.127 | Gen: - airaywy onditingcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.028 | Val loss: 0.107 | Gen: - airaywy onditingcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.027 | Val loss: 0.121 | Gen: - array onditingcay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.025 | Val loss: 0.127 | Gen: ethy iirrry onditingcay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.026 | Val loss: 0.101 | Gen: - iirayay onditingcay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.031 | Val loss: 0.140 | Gen: ethy airaaay ondttnngcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.051 | Val loss: 0.173 | Gen: ethay airay onditingcy isway orkingway\n",
            "Epoch:  56 | Train loss: 0.087 | Val loss: 0.176 | Gen: - irway onditigcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.068 | Val loss: 0.117 | Gen: ethaEOSy airay ondiongcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.047 | Val loss: 0.135 | Gen: ethy airay onditingcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.035 | Val loss: 0.123 | Gen:  airay ondtongcayy isway orkingway\n",
            "Epoch:  60 | Train loss: 0.033 | Val loss: 0.138 | Gen: ethy airwaEOSmy onditingccay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.019 | Val loss: 0.089 | Gen: ethy airay oddiiigi isway orkingway\n",
            "Epoch:  62 | Train loss: 0.012 | Val loss: 0.082 | Gen: ethy airaay onditingcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.013 | Val loss: 0.100 | Gen: ethy airaay onditingcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.012 | Val loss: 0.094 | Gen: ethy airaay onditingcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.011 | Val loss: 0.094 | Gen: ethy airwaaay ondioniongy isway orkingway\n",
            "Epoch:  66 | Train loss: 0.009 | Val loss: 0.085 | Gen: ethy airwaEOSmy onditingcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.005 | Val loss: 0.071 | Gen: ethy airwaEOSmy onditingcay iaay orkingway\n",
            "Epoch:  68 | Train loss: 0.004 | Val loss: 0.085 | Gen: ethy airayy onditingcay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.006 | Val loss: 0.076 | Gen: ethy airwaEOSmy onditingcay iaay orkingway\n",
            "Epoch:  70 | Train loss: 0.123 | Val loss: 0.208 | Gen: ethy iirayy onditignayygyEOSEOSmty isway orkingway\n",
            "Epoch:  71 | Train loss: 0.139 | Val loss: 0.171 | Gen: ehay airray onditingcay iswayy orkingway\n",
            "Epoch:  72 | Train loss: 0.086 | Val loss: 0.146 | Gen: ehay iirayy onditiongcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.038 | Val loss: 0.077 | Gen: ehhy airwy onditiongcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.016 | Val loss: 0.088 | Gen: ehay airwy onditingcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.017 | Val loss: 0.087 | Gen: ehhy airwy onditiongcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.019 | Val loss: 0.079 | Gen: ehhy iiway onditiongcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.020 | Val loss: 0.091 | Gen: ehhy iiway onditiongcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.020 | Val loss: 0.090 | Gen: ehhy airaEOSEOSry onditiongcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.015 | Val loss: 0.095 | Gen: ehhy airwaEOSray onditiongcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.011 | Val loss: 0.075 | Gen: ehhyyy airwaEOSray onditingcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.020 | Val loss: 0.105 | Gen: ehhy airwaEOSway onditingcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.039 | Val loss: 0.184 | Gen: ehhy airay onditinggcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.085 | Val loss: 0.153 | Gen: ththwy airaiay ondionionncay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.057 | Val loss: 0.158 | Gen: ethy airwy onditiongcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.049 | Val loss: 0.118 | Gen: etay airway onditiiongcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.022 | Val loss: 0.089 | Gen: ethyhy airwy onditioningy isway orkingway\n",
            "Epoch:  87 | Train loss: 0.006 | Val loss: 0.086 | Gen: ehay airwy onditiongcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.004 | Val loss: 0.084 | Gen: ehay airwy onditiongcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.009 | Val loss: 0.084 | Gen: ethy airwy onditiongcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.003 | Val loss: 0.083 | Gen: ehay airwy onditiongcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.003 | Val loss: 0.096 | Gen: ehay airwy onditiongcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.008 | Val loss: 0.084 | Gen: ehay airwy onditiongcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.006 | Val loss: 0.093 | Gen: ehay airwy onditiongcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.005 | Val loss: 0.085 | Gen: ehay airwy onditioningcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.002 | Val loss: 0.079 | Gen: ehay airwy onditiongcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.001 | Val loss: 0.083 | Gen: ehay airwy onditiongcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.001 | Val loss: 0.083 | Gen: ehay airwy onditiongcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.001 | Val loss: 0.083 | Gen: ehay airwy onditiongcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.001 | Val loss: 0.083 | Gen: ehay airwy onditiongcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehay airwy onditiongcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ULCMHm5ZF7vx",
        "colab_type": "code",
        "outputId": "2239a580-1ba2-4c51-fa4d-3aa649e1b65a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airry onditioningcay isway orkingay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qbfZCByITOI6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Attention visualization"
      ]
    },
    {
      "metadata": {
        "id": "itCGMv3FdXsn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEST_WORD_ATTN = 'man'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xBv4QQuBiU-V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize RNN attention map"
      ]
    },
    {
      "metadata": {
        "id": "aXvqoQYONMTA",
        "colab_type": "code",
        "outputId": "aaeae2cb-1931-4c2c-bb2e-ed06b6e87323",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "cell_type": "code",
      "source": [
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgwAAAGACAYAAADBHDoxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'anmay'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "metadata": {
        "id": "xuOvxfA1NMz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize transformer attention maps from all the transformer layers"
      ]
    },
    {
      "metadata": {
        "id": "HSSB4wd8-M7g",
        "colab_type": "code",
        "outputId": "946f11a2-e34f-4fe1-bf00-3c3cde690a86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1186
        }
      },
      "cell_type": "code",
      "source": [
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgwAAAGACAYAAADBHDoxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh1mWXAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAGACAYAAAAjwCFIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBxjJRNC/aJc1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAGACAYAAAAjwCFIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'anmay'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    }
  ]
}